{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT NO. 1\n",
    "\n",
    "# TITLE: VANILLA\n",
    "\n",
    "# OBJECTIVE: Verify the working of DQN for Tokyo 2010\n",
    "\n",
    "# PROCEDURE:  Train using Tokyo,2010 for 50 iterations - make 3 simultaneous copies\n",
    "#             NO Shuffling of days when training\n",
    "#             REMOVE FORECAST STATE\n",
    "#             Test using Tokyo, 2010 using greedy policy and best trained model\n",
    "#             Test using Tokyo, 2011 using greedy policy and best trained model\n",
    "#             Test using Wakkanai, 2011 using greedy policy and best trained model\n",
    "#             Test using Minamidaito, 2011 using greedy policy and best trained model\n",
    "#             Test using Tokyo, 2010 using greedy policy and best trained model\n",
    "\n",
    "#SETUP : \n",
    "#Battery      => 3.7V x 2500mAh = 9250 mWh, [MODEL: 785060 2500mAh]\n",
    "#Solar Panel  => 500mW [55mm x 70mm, ECCN-3A991.o    HSCODE-8541402000] \n",
    "#Node         => 50mw to 500 mW, 10 duty cycles [DigiXBee3 PRO ZigBee 3.0]\n",
    "\n",
    "# RESULTS:\n",
    "# COPY1 showed the best results. All other results were pretty unstable even after 50 iterations of training\n",
    "\n",
    "# Higher average rewards doesn't necessarily mean better performance - COPY1 had lower average rewards \n",
    "# for MINAMIDAITO as compared to COPY2  but better performance.\n",
    "# This maybe because the batter levels fluctuate around different levels. \n",
    "# Fluctating around a higher level (70%) means lower rewards but fluctuating around lower levels (50%) means riskier policies \n",
    "\n",
    "# A better comparison metric is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #matrix with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        file = './data/' + self.location +'/' + str(self.year) + '.csv'\n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(file, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "        \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        sradiation = solar_radiation.reshape(-1,24)\n",
    "        #convert missing data in CSV files to zero\n",
    "        sradiation[np.isnan(sradiation)] = 0\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(sradiation) \n",
    "        self.sradiation = sradiation\n",
    "        \n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "\n",
    "        self.senergy = self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60) \n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        if (tot_day_radiation < 3.5):\n",
    "            day_state = 0\n",
    "        elif (3.5 <= tot_day_radiation < 7):\n",
    "            day_state = 1\n",
    "        elif (7 <= tot_day_radiation < 12):\n",
    "            day_state = 2\n",
    "        elif (12 <= tot_day_radiation < 15):\n",
    "            day_state = 3\n",
    "        elif (15 <= tot_day_radiation < 17.5):\n",
    "            day_state = 4\n",
    "        else:\n",
    "            day_state = 5\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "\n",
    "        if(self.hr < self.TIME_STEPS - 1):\n",
    "            self.hr += 1\n",
    "            self.henergy = self.senergy[self.day][self.hr] \n",
    "        else:\n",
    "            if(self.day < self.NO_OF_DAYS -1):\n",
    "                end_of_day = True\n",
    "                self.hr = 0\n",
    "                self.day += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                end_of_day = True\n",
    "                end_of_year = True\n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "\n",
    "\n",
    "#Continuous Adaptive Power Manager using default ENO class\n",
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = 500   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = ENO(self.location, self.year, shuffle)\n",
    "\n",
    "        self.no_of_day_state = 6;\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.enp = self.BOPT - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "#         norm_fcast = self.fcast/(self.no_of_day_state-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "#         norm_fcast = self.fcast/(self.no_of_day_state-1)\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "\n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        R_PARAM = 20000 #chosen empirically for best results\n",
    "        mu = 0\n",
    "        sig = 0.05*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "        \n",
    "        if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "            norm_reward = (np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "        else:\n",
    "            norm_reward = -0.25 - 2.5*np.abs(self.enp/R_PARAM)\n",
    "    \n",
    "        return norm_reward\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        reward = 0\n",
    "        \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        \n",
    "        #code to record all the times battery levels have been exceeded\n",
    "        self.enp = self.BOPT - self.batt\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "\n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward = self.rewardfn()\n",
    "            \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "                if(self.batt == self.BMIN or self.batt == self.BMAX ):\n",
    "                    self.batt = self.BOPT\n",
    "                    reward = reward - 2 #penalty for violating battery limits\n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/5\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(230228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definitions for NN model and learning algorithm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        \n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "    \n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] # return the argmax index\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "LOCATION = 'tokyo'\n",
    "YEAR = 2010\n",
    "\n",
    "BATCH_SIZE = 24\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*2    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY = 24*7*4*6      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 3 #number of state space parameter [batt, enp, henergy]\n",
    "HIDDEN_LAYER = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check reward function\n",
    "\n",
    "# test = CAPM(LOCATION,YEAR,shuffle=False) #refer to eno_class.py class file\n",
    "# y1 = np.empty(1)\n",
    "# for x in np.arange(-test.BMAX/2,test.BMAX/2):\n",
    "#     test.enp = x\n",
    "#     y1 = np.append(y1,test.rewardfn())\n",
    "# y1 = np.delete(y1, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "# enp = np.arange(-test.BMAX/2,test.BMAX/2)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(enp, y1)\n",
    "\n",
    "# ax.set(xlabel='ENP', ylabel='Reward',\n",
    "#        title='Reward Function')\n",
    "# ax.grid()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting experience... Iteration: 0\n",
      "End of Year\n",
      "Average reward = -1.8119371192612066\n",
      "\n",
      "Collecting experience... Iteration: 1\n",
      "End of Year\n",
      "Average reward = 0.10633294746378613\n",
      "\n",
      "Collecting experience... Iteration: 2\n",
      "End of Year\n",
      "Average reward = 0.31263442540159914\n",
      "\n",
      "Collecting experience... Iteration: 3\n",
      "End of Year\n",
      "Average reward = 0.28370664422794495\n",
      "\n",
      "Collecting experience... Iteration: 4\n",
      "End of Year\n",
      "Average reward = 0.052721906929907025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe43c4b1e10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TRAIN USING TOKYO 2010 data\n",
    "dqn = DQN() #refer to learner_class.py class file\n",
    "capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=True) #refer to eno_class.py class file\n",
    "\n",
    "NO_OF_ITERATIONS = 50\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "PFILENAME = PFILENAME + \".pt\" #this file stores the best model\n",
    "\n",
    "avg_reward_rec = np.empty(1)\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "\n",
    "    print('\\nCollecting experience... Iteration:', iteration)\n",
    "#     print(\"EPSILON = \", EPSILON)\n",
    "#     print(\"LR = \", LR)\n",
    "#     print(\"LAMBDA = \", LAMBDA)\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    record = np.empty(4)\n",
    "\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(s)\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        temp_transitions = np.hstack((s, [a, r], s_))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] = r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if (year_end):\n",
    "            print(\"End of Year\")\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "    reward_rec = record[:,2]\n",
    "    reward_rec = reward_rec[reward_rec != 0]\n",
    "    print(\"Average reward =\", np.mean(reward_rec) )\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "\n",
    "    if (iteration > 20): #save the best models only after 20 iterations\n",
    "        if(best_avg_reward < np.mean(reward_rec)):\n",
    "            best_avg_reward = np.mean(reward_rec)\n",
    "            print(\"Better reward -> \", best_avg_reward)\n",
    "            print(\"Saving model\")\n",
    "            torch.save(dqn.eval_net.state_dict(), PFILENAME)\n",
    "\n",
    "avg_reward_rec = np.delete(avg_reward_rec, 0, 0) #remove the first row which is garbage\n",
    "plt.plot(avg_reward_rec,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(\"TOKYO, 2010  DAY %s\" %(DAY))\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"Harvested Energy\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1.2])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.set_title(\"YEAR RUN\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(12, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=11, ha='center')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year run test\n",
      "Model Used:  PC1FK85I.pt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PC1FK85I.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7df606cd9c14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPFILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PC1FK85I.pt'"
     ]
    }
   ],
   "source": [
    "#Test the trained model for TOKYO 2010\n",
    "# dqn = DQN()\n",
    "capm = CAPM('tokyo',2010, shuffle=False, trainmode=True)\n",
    "print('\\nYear run test')\n",
    "print('Model Used: ',PFILENAME)\n",
    "\n",
    "#load the best model\n",
    "dqn.eval_net.load_state_dict(torch.load(PFILENAME))\n",
    "dqn.eval_net.eval()\n",
    "s, r, day_end, year_end = capm.reset()\n",
    "yr_test_record = np.empty(4)\n",
    "\n",
    "while True:\n",
    "    a = dqn.choose_greedy_action(s)\n",
    "\n",
    "    #state = [batt, enp, henergy, fcast]\n",
    "    yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "    # take action\n",
    "    s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "    if year_end:\n",
    "        print(\"End of Year\")\n",
    "        break\n",
    "       \n",
    "    s = s_\n",
    "\n",
    "yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "\n",
    "\n",
    "#Plot the reward and battery for the entire year run\n",
    "NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "yr_test_reward_rec = yr_test_record[:,2]\n",
    "yr_test_reward_rec = yr_test_reward_rec[yr_test_reward_rec != 0]\n",
    "print('Average Reward = ', np.mean(yr_test_reward_rec))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(24,10))\n",
    "fig.suptitle('TOKYO 2010', fontsize=15)\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(yr_test_reward_rec)\n",
    "ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "ax1.set_ylim([-3,3])\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.plot(yr_test_record[:,0],'r')\n",
    "ax2.set_title(\"Year Run Battery\")\n",
    "ax2.set_ylim([0,1])\n",
    "plt.sca(ax2)\n",
    "plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the trained model for TOKYO 2011\n",
    "# dqn = DQN()\n",
    "capm = CAPM('tokyo',2011, shuffle=False, trainmode=True)\n",
    "print('\\nYear run test')\n",
    "print('Model Used: ',PFILENAME)\n",
    "\n",
    "#load the best model\n",
    "dqn.eval_net.load_state_dict(torch.load(PFILENAME))\n",
    "dqn.eval_net.eval()\n",
    "s, r, day_end, year_end = capm.reset()\n",
    "yr_test_record = np.empty(4)\n",
    "\n",
    "while True:\n",
    "    a = dqn.choose_greedy_action(s)\n",
    "\n",
    "    #state = [batt, enp, henergy, fcast]\n",
    "    yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "    # take action\n",
    "    s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "    if year_end:\n",
    "        print(\"End of Year\")\n",
    "        break\n",
    "       \n",
    "    s = s_\n",
    "\n",
    "yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "\n",
    "\n",
    "#Plot the reward and battery for the entire year run\n",
    "NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "yr_test_reward_rec = yr_test_record[:,2]\n",
    "yr_test_reward_rec = yr_test_reward_rec[yr_test_reward_rec != 0]\n",
    "print('Average Reward = ', np.mean(yr_test_reward_rec))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(24,15))\n",
    "fig.suptitle('TOKYO 2011', fontsize=15)\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(yr_test_reward_rec)\n",
    "ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "ax1.set_ylim([-3,3])\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.plot(yr_test_record[:,0],'r')\n",
    "ax2.set_title(\"Year Run Battery\")\n",
    "ax2.set_ylim([0,1])\n",
    "plt.sca(ax2)\n",
    "plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(\"TOKYO, 2011  DAY %s\" %(DAY))\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"Harvested Energy\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1.2])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.set_title(\"YEAR RUN\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(12, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=11, ha='center')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the trained model for WAKKANAI, 2011\n",
    "# dqn = DQN()\n",
    "capm = CAPM('wakkanai',2011, shuffle=False, trainmode=True)\n",
    "print('\\nYear run test')\n",
    "print('Model Used: ',PFILENAME)\n",
    "\n",
    "#load the best model\n",
    "dqn.eval_net.load_state_dict(torch.load(PFILENAME))\n",
    "dqn.eval_net.eval()\n",
    "s, r, day_end, year_end = capm.reset()\n",
    "yr_test_record = np.empty(4)\n",
    "\n",
    "while True:\n",
    "    a = dqn.choose_greedy_action(s)\n",
    "\n",
    "    #state = [batt, enp, henergy, fcast]\n",
    "    yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "    # take action\n",
    "    s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "    if year_end:\n",
    "        print(\"End of Year\")\n",
    "        break\n",
    "       \n",
    "    s = s_\n",
    "\n",
    "yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "\n",
    "\n",
    "#Plot the reward and battery for the entire year run\n",
    "NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "yr_test_reward_rec = yr_test_record[:,2]\n",
    "yr_test_reward_rec = yr_test_reward_rec[yr_test_reward_rec != 0]\n",
    "print('Average Reward = ', np.mean(yr_test_reward_rec))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(24,10))\n",
    "fig.suptitle('WAKKANAI 2011', fontsize=15)\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(yr_test_reward_rec)\n",
    "ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "ax1.set_ylim([-3,3])\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.plot(yr_test_record[:,0],'r')\n",
    "ax2.set_title(\"Year Run Battery\")\n",
    "ax2.set_ylim([0,1])\n",
    "plt.sca(ax2)\n",
    "plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(\"WAKKANAI, 2011  DAY %s\" %(DAY))\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"Harvested Energy\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1.2])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.set_title(\"YEAR RUN\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(12, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=11, ha='center')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the trained model for MINAMIDAITO, 2011\n",
    "# dqn = DQN()\n",
    "capm = CAPM('minamidaito',2011, shuffle=False, trainmode=True)\n",
    "print('\\nYear run test')\n",
    "print('Model Used: ',PFILENAME)\n",
    "\n",
    "#load the best model\n",
    "dqn.eval_net.load_state_dict(torch.load(PFILENAME))\n",
    "dqn.eval_net.eval()\n",
    "s, r, day_end, year_end = capm.reset()\n",
    "yr_test_record = np.empty(4)\n",
    "\n",
    "while True:\n",
    "    a = dqn.choose_greedy_action(s)\n",
    "\n",
    "    #state = [batt, enp, henergy, fcast]\n",
    "    yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "    # take action\n",
    "    s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "    if year_end:\n",
    "        print(\"End of Year\")\n",
    "        break\n",
    "       \n",
    "    s = s_\n",
    "\n",
    "yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "\n",
    "\n",
    "#Plot the reward and battery for the entire year run\n",
    "NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "yr_test_reward_rec = yr_test_record[:,2]\n",
    "yr_test_reward_rec = yr_test_reward_rec[yr_test_reward_rec != 0]\n",
    "print('Average Reward = ', np.mean(yr_test_reward_rec))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(24,10))\n",
    "fig.suptitle('MINAMIDAITO 2011', fontsize=15)\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(yr_test_reward_rec)\n",
    "ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "ax1.set_ylim([-3,3])\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.plot(yr_test_record[:,0],'r')\n",
    "ax2.set_title(\"Year Run Battery\")\n",
    "ax2.set_ylim([0,1])\n",
    "plt.sca(ax2)\n",
    "plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the reward and battery for the entire year run on a day by day basis\n",
    "\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(155,170):#range(capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(\"MINAMIDAITO, 2011  DAY %s\" %(DAY))\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"Harvested Energy\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1.2])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.set_title(\"YEAR RUN\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(12, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=11, ha='center')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
